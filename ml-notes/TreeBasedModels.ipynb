{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tree-Based Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- It has been observed that tree based models have been able to map non-linearity effectively.\n",
    "- The tree-based models can only interpolate but not extrapolate. For example, a decision tree makes constant prediction for the objects that lie beyond the bounding box set by the training set in the feature space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- At the heart of the popular algorithms for decision tree construction lies the principle of greedy maximization of information gain: \n",
    "    - At each step the algorithm choose the variable that gives the greatest information gain upon splitting. \n",
    "    <center><img width=350 src=\"images/shannon_equation.jpg\"/></center>\n",
    "    <center><img width=400 src=\"images/Image-12-Equation-600x158.png\"/></center>\n",
    "    - Then the procedure is repeated recursively until the entropy is zero (or some small value to account for overfitting).\n",
    "    <center><img width=250 src=\"images/decision-tree.svg\"/></center>\n",
    "    <center><a href=\"https://victorzhou.com/blog/intro-to-random-forests/\" style=\"color: lightgrey\">Credit</a></center>\n",
    "- The simplest heuristics for handling numeric features in a decision tree is to sort its values in ascending order and check only those thresholds where the value of the target variable changes. The value obtained by leaves in the training data is the mean response of observation falling in that region. \n",
    "- Criterions: In practice, Gini uncertainty and information gain work similarly.\n",
    "- [Introduction to Decision Trees](https://medium.com/greyatom/decision-trees-a-simple-way-to-visualize-a-decision-dc506a403aeb)\n",
    "- [A Simple Guide to Entropy-Based Discretization](https://natmeurer.com/a-simple-guide-to-entropy-based-discretization/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tuning:\n",
    "- *max_depth*: the maximum depth of the tree\n",
    "- *max_features*: the maximum number of features with which to search for the best partition (this is necessary with a large number of features because it would be \"expensive\" to search for partitions for all features)\n",
    "- *min_samples_leaf*: the minimum number of samples in a leaf. This parameter prevents creating trees where any leaf would have only a few members\n",
    "- Pruning trees. In this approach, the tree is first constructed to the maximum depth. Then, from the bottom up, some nodes of the tree are removed by comparing the quality of the tree with and without that partition (comparison is performed using cross-validation, more on this below)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Benefits:\n",
    "- Graphical representation is very intuitive and users can easily relate their hypothesis\n",
    "- Decision tree is one of the fastest way to identify most significant variables and relation between two or more variables\n",
    "- Decision trees implicitly perform variable screening or feature selection\n",
    "- Data type is not a constraint: It can handle both numerical and categorical variables. Can also handle multi-output problems.\n",
    "- Non-linear relationships between parameters do not affect tree performance\n",
    "- Fast training and forecasting\n",
    "- Small number of hyper-parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Drawbacks:\n",
    "- Overfitting: Decision-tree learners can create over-complex trees that do not generalize the data well.\n",
    "- Not fit for continuous variables: While working with continuous numerical variables, decision tree loses information, when it categorizes variables in different categories.\n",
    "- Small changes to the data can significantly change the decision tree. Decision trees can be unstable because small variations in the data might result in a completely different tree being generated. This is called variance, which needs to be lowered by methods like bagging and boosting.\n",
    "- Decision tree learners create biased trees if some classes dominate\n",
    "- The optimal decision tree search problem is NP-complete\n",
    "- Generally, it gives low prediction accuracy for a dataset as compared to other machine learning algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Random forests are combined through the construction of uncorrelated trees using CART, bagging, and the random subspace method.\n",
    "\n",
    "<img width=300 src=\"images/random-forests.jpg\"/>\n",
    "<center><a href=\"https://blog.lokad.com/journal/2019/1/11/columnar-random-forests/\" style=\"color: lightgrey\">Credit</a></center>\n",
    "\n",
    "- Bias-variance tradeoff: As random forests training use 1) bootstrap sampling (or sampling with replacement) along with 2) random selection of features for a split, the correlation between the trees (or weak learners) would be low. That means although individual trees would have high variance but the ensemble output will be appropriate (lower variance but the same bias) because the trees are loosely correlated. Thus random forests would give good performance with full depth of decision trees.\n",
    "\n",
    "<img width=800 src=\"images/tree_vs_bagging_eng.png\"/>\n",
    "<center><a href=\"https://github.com/Yorko/mlcourse.ai/blob/master/jupyter_english/topic05_ensembles_random_forests/topic5_part1_bagging.ipynb\" style=\"color: lightgrey\">Credit</a></center>\n",
    "\n",
    "- After training, predictions for unseen samplescan be made by averaging the predictions from all the individual regression trees or by taking the majority vote in the case of classification trees.\n",
    "- Random forests are mostly used in supervised learning, but there is a way to apply them in the unsupervised setting: Using *RandomTreesEmbedding*, we can transform our dataset into a high-dimensional, sparse representation. We first build extremely randomized trees and then use the index of the leaf containing the example as a new feature. Under the hood, calculates proximities between pairs of examples that can subsequantly be used in clustering, outlier detection, or interesting data representations.\n",
    "- [Ensembles and Random Forests: A series of notebooks](https://github.com/Yorko/mlcourse.ai/tree/master/jupyter_english/topic05_ensembles_random_forests)\n",
    "- [Feature Selection Using Random Forests](https://towardsdatascience.com/feature-selection-using-random-forest-26d7b747597f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tuning:\n",
    "- *n_estimators*: the number of trees in the forest. An optimal number of trees can be found using cross-validation, or by observing the out-of-bag error (controlled by *oob_score*): the mean prediction error on each training sample, using only the trees that did not have it in their bootstrap sample.\n",
    "- *criterion*: the function used to measure the quality of a split\n",
    "- *max_features* $m$: the number of features to consider when looking for the best split\n",
    "- *min_samples_leaf* $n_{min}$: the minimum number of samples required to be at a leaf node\n",
    "- *max_depth*: the maximum depth of the tree\n",
    "- For classification problems, it is advisable to set $m=\\sqrt{d}$. For regression problems, we usually take $m=\\frac{d}{3}$, where $d$ is the number of features. It is recommended to build each tree until all of its leaves contain only $n_\\text{min}=1$ examples for classification and $n_\\text{min} = 5$ examples for regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Benefits:\n",
    "- High prediction accuracy; will perform better than linear algorithms in most problems\n",
    "- Robust to outliers, thanks to random sampling\n",
    "- Insensitive to the scaling of features as well as any other monotonic transformations due to the random subspace selection\n",
    "- Doesn't require fine-grained parameter tuning, works quite well out-of-the-box\n",
    "- Efficient for datasets with a large number of features and classes\n",
    "- Handles both continuous and discrete variables equally well\n",
    "- Rarely overfits. In practice, an increase in the tree number almost always improves the composition. But, after reaching a certain number of trees, the learning curve is very close to the asymptote\n",
    "\n",
    "<img width=450 src=\"images/performance_rf.png\"/>\n",
    "<center><a href=\"https://www.researchgate.net/publication/332391062_Wet_and_Dry_Snow_Detection_Using_Sentinel-1_SAR_Data_for_Mountainous_Areas_with_a_Machine_Learning_Technique/figures?lo=1&utm_source=google&utm_medium=organic\" style=\"color: lightgrey\">Credit</a></center>\n",
    "\n",
    "- There are developed methods to estimate feature importance\n",
    "- Works well with missing data and maintains good accuracy levels even when a large part of data is missing\n",
    "- Easily parallelized and highly scalable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Drawbacks:\n",
    "- The main limitation of Random Forest is that a large number of trees can make the algorithm to slow and ineffective for real-time predictions.\n",
    "- There are no formal $p$-values for feature significance estimation\n",
    "- Performs worse than linear methods in the case of sparse data: text inputs, bag of words, etc.\n",
    "- Unlike linear regression, Random Forest is unable to extrapolate. But, this can be also regarded as an advantage because outliers do not cause extreme values in Random Forests.\n",
    "- The resulting model is large and requires a lot of RAM\n",
    "- Overfitting:\n",
    "    - Prone to overfitting in some problems, especially, when dealing with noisy data.\n",
    "    - In the case of categorical variables with varying level numbers, random forests favor variables with a greater number of levels. The tree will fit more towards a feature with many levels because this gains greater accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boosting trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In contrast to random forests, boosting is an approach to increase the complexity of models that suffer from high bias, that is, models that underfit the training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adaptive boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
