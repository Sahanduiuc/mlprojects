{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- An ensemble method combines the predictions of many individual classifiers by majority voting.\n",
    "- Ensemble of *low-correlating* classifiers with slightly greater than 50% accuracy will outperform each of the classifiers individually.\n",
    "- Condorcet's jury theorem: \n",
    "    - If each member of the jury (of size $N$) makes an *independent* judgement and the probability $p$ of the correct decision by each juror is more than 0.5, then the probability of the correct decision $P_N$ by the majority $m$ tends to one. On the other hand, if $p<0.5$ for each juror, then the probability tends to zero.\n",
    "    <center><img width=350 src=\"images/Screen Shot 2019-06-06 at 20.29.13.png\"/></center>\n",
    "    - where $m$ as a minimal number of jurors that would make a majority.\n",
    "    - But real votes are not independent, and do not have uniform probabilities.\n",
    "- Uncorrelated submissions clearly do better when ensembled than correlated submissions.\n",
    "- Majority votes make most sense when the evaluation metric requires hard predictions.\n",
    "- [KAGGLE ENSEMBLING GUIDE](https://mlwave.com/kaggle-ensembling-guide/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Averaging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Averaging is taking the mean of individual model predictions.\n",
    "- Averaging predictions often reduces variance (as bagging does).\n",
    "- Itâ€™s a fairly trivial technique that results in easy, sizeable performance improvements.\n",
    "- Averaging exactly the same linear regressions won't give any penalty.\n",
    "- An often heard shorthand for this on Kaggle is \"bagging submissions\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Weighted averaging:\n",
    "- Use weighted averaging to give a better model more weight in a vote."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conditional averaging:\n",
    "- Use conditional averaging to cancel out erroneous ranges of individual estimators."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Bagging (bootstrap aggregating) means averaging slightly different versions of the same model to improve accuracy.\n",
    "- Bagging combines *strong learners* together in order to \"smooth out\" their predictions and reduce overfitting.\n",
    "- Randomly subsampled training data (bootstrapping) produces further diversity of individual models and drives efficiency.\n",
    "\n",
    "<center><img width=350 src=\"images/Ozone.png\"/></center>\n",
    "<center><a href=\"https://en.wikipedia.org/wiki/Bootstrap_aggregating\" style=\"color: lightgrey\">Credit</a></center>\n",
    "\n",
    "\n",
    "- Can be used with any type of method as a base model.\n",
    "- Bagging is effective on small datasets.\n",
    "- Out-of-bag estimate is the mean estimate of the base algorithms on 37% of inputs that are left out of a particular bootstrap sample.\n",
    "    - Helps avoid the need for an independent validation dataset.\n",
    "- Parameters that control bagging:\n",
    "    - Random seed\n",
    "    - Row sampling or bootstrapping\n",
    "    - Column sampling or bootstrapping\n",
    "    - Size of sample (use a much smaller sample size on a larger dataset)\n",
    "    - Shuffling\n",
    "    - Number of bags\n",
    "    - Parallelism"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bootstrapping:\n",
    "- Bootstrapping is random sampling with replacement.\n",
    "- Sampling with replacement is a convenient way to treat a sample like it is a population.\n",
    "- This technique allows estimation of the sampling distribution of almost any statistic using random sampling methods.\n",
    "- It is a straightforward way to derive estimates of standard errors and confidence intervals for complex estimators.\n",
    "- For example:\n",
    "    - Select a random element from the original sample of size $N$ and do this $N$ times.\n",
    "    - Calculate the mean of each sub-sample.\n",
    "    - Obtain a 95% confidence interval around the mean estimate for the original sample.\n",
    "- Each bootstrap sample of size $N$ contains on average $0.623*N$ observations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
