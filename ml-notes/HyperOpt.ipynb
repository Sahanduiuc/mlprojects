{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Hyperparameters are model-specific properties that are not learned (as parameters) but fixed.\n",
    "- Hyperparameter tuning is a meta-optimization task.\n",
    "- Quality of the hyperparameters is not deterministic, as it depends on the outcome of a black box (the model training process).\n",
    "    - We can't obtain the derivative and thus can't apply other mathematical optimization tools.\n",
    "- [Tuning the hyper-parameters of an estimator (sklearn)](http://scikit-learn.org/stable/modules/grid_search.html)\n",
    "- [A list of open-source software](https://en.wikipedia.org/wiki/Hyperparameter_optimization#Open-source_software)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tips:\n",
    "- Select a subset of the most influential hyperparameters:\n",
    "    - There are tons of hyperparameters and there is no time to tune them all.\n",
    "    - [Multiple comparisons problem](https://en.wikipedia.org/wiki/Multiple_comparisons_problem#cite_note-2): The more inferences are made, the more likely erroneous inferences are to occur, because of the sheer size of the parameter space to be searched.\n",
    "- Understand exactly how they influence the training.\n",
    "- Find out if the model is underfitting or overfitting.\n",
    "- Use cross-validation to estimate the generalization performance.\n",
    "- Don't spend too much time tuning hyperparameters.\n",
    "    - Only if you have no more ideas or you have spare computational resources.\n",
    "    - You cannot win competitions by tuning (but with features, hacks, leaks, and insights).\n",
    "- It can take thousands of rounds for GBDT or neural networks to fit.\n",
    "- Average everything:\n",
    "    - Over random seed\n",
    "    - Over small deviations from optimal parameters (e.g. average `max_depth`=3,4,5 for an optimal 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Try every combination of a preset list of hyper-parameters and evaluate the model for each combination.\n",
    "- The list of combinations is calculated as a Cartesian product of different hyperparameter sets.\n",
    "- Manually set bounds and discretization may be necessary before applying grid search.\\\n",
    "- Manual grid search:\n",
    "    - Run a small grid, see if the optimum lies at either endpoint, and then expand the grid in that direction.\n",
    "- Grid search is simple to set up and trivial to parallelize.\n",
    "- It is the most expensive method in terms of total computation time. \n",
    "- However, if run in parallel, it is fast in terms of wall clock time.\n",
    "- Suffers from the curse of dimensionality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Random search only evaluates a random sample of points on the grid.\n",
    "- This method is more efficient for parameter optimization than grid search.\n",
    "    - While grid search captures grid points only, random search is free to search the whole action space (without any aliasing).\n",
    "    - It is empirically and theoretically shown, that if at least 5% of the points on the grid yield a close-to-optimal solution, then random search with 60 trials will find that region with 95% probability (and the close-to-optimal region in stable machine learning models is quite large).\n",
    "    - Compared to other methods it doesn't bog down in local optima.\n",
    "    - Random search allows the inclusion of prior knowledge by specifying the distribution from which to sample.\n",
    "    <center><img width=500 src=\"images/both.png\"/></center>\n",
    "    <center><a href=\"https://www.analyticsindiamag.com/why-is-random-search-better-than-grid-search-for-machine-learning/\" style=\"color: lightgrey\">Credit</a></center>\n",
    "- Works best when: \n",
    "    - Only a small number of hyperparameters affects the final performance.\n",
    "    - There are less number of dimensions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Smart search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The goal is to converge faster and make fewer evaluations.\n",
    "- This type of methods is rarely parallelizable.\n",
    "- Makes sense only if the evaluation procedure takes much longer than the sampling process.\n",
    "- Smart search algorithms contain hyperparameters of their own."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bayesian optimization:\n",
    "- Bayesian Optimization uses all of the information from previous evaluations and determines the next point to try.\n",
    "- Builds a probabilistic model of the function mapping from the hyperparameters to the objective.\n",
    "\n",
    "<img width=350 src=\"images/6a010534b1db25970b01b7c869bebc970b-800wi.gif\"/>\n",
    "<center><a href=\"https://blog.revolutionanalytics.com/2016/06/bayesian-optimization-of-machine-learning-models.html\" style=\"color: lightgrey\">Credit</a></center>\n",
    "\n",
    "- Bayesian optimization obtain better results in fewer evaluations compared to grid search and random search.\n",
    "- Bayesian optimization is much better than manual tuning.\n",
    "- It is well suited for functions that are expensive to evaluate.\n",
    "- Computes the mean and the variance.\n",
    "- Function evaluation is cubic on the number of inputs.\n",
    "    - Use a deterministic neural network with Bayesian linear regression on the last hidden layer.\n",
    "    - Bayesian linear regression is much faster than Spearmint.\n",
    "- Using Gaussian processes: [Spearmint](https://github.com/HIPS/Spearmint)\n",
    "- Using Tree-based Parzen Estimators: [Hyperopt](http://hyperopt.github.io/hyperopt/)\n",
    "    - [Optimizing hyperparams with hyperopt](http://fastml.com/optimizing-hyperparams-with-hyperopt/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SMAC (Sequential Model-based Algorithm Configuration):\n",
    "- Trains a random forest of regression trees to approximate the response surface.\n",
    "- This method may work better than Gaussian processes for categorical hyperparameters.\n",
    "- Random forest tuning: [SMAC](http://bit.ly/SMAC-proj)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Derivative-free optimization:\n",
    "- Try a bunch of random points, approximate the gradient, and find the most likely search direction.\n",
    "- Derivative-free methods include genetic algorithms and the Nelder-Mead method.\n",
    "- Easy to implement and no less efficient that Bayesian optimization.\n",
    "- Hyper gradient: [hypergrad](https://github.com/HIPS/hypergrad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
