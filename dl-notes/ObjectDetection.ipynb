{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Object Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The object detection task consists in determining the location on the image where certain objects are present, as well as classifying those objects\n",
    "- Some of the most important elements that are now staple in most of state-of-the art algorithms include inception and residual blocks, skip connections and upsampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## YOLO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### YOLOv1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- YOLO (\"you only look once\") is a popular algoritm because it achieves high accuracy while also being able to run in real-time. \n",
    "- This algorithm \"only looks once\" at the image in the sense that it requires only one forward propagation pass through the network to make predictions. \n",
    "- After non-max suppression, it then outputs recognized objects together with the bounding boxes.\n",
    "- YOLO uses a single CNN network for both classification and localizing the object using bounding boxes.\n",
    "\n",
    "<img width=250 src=\"images/yologo_2.png\"/>\n",
    "<center><a href=\"https://pjreddie.com/darknet/yolo/\" style=\"color: lightgrey\">Credit</a></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Segmentation:\n",
    "- The input image is divided into an SxS grid of cells. For each object that is present on the image, one grid cell is said to be “responsible” for predicting it. That is the cell where the center of the object falls into.\n",
    "- The division is done with the convolution sliding window, which is more efficient than classical sliding windows detection algorithm, because it shares the computations for each of the sliding windows\n",
    "\n",
    "<img width=800 src=\"images/21.png\"/>\n",
    "<center><a href=\"https://www.coursera.org/learn/convolutional-neural-networks\" style=\"color: lightgrey\">Credit</a></center>\n",
    "\n",
    "- Unlike sliding window and region proposal-based techniques, YOLO sees the entire image during training and test time so it implicitly encodes contextual information about classes as well as their appearance. \n",
    "- The (effective) receptive field of those output neurons is much larger than the cell and actually cover the entire image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prediction:\n",
    "- YOLO predicts multiple bounding boxes per grid cell\n",
    "- A bounding box describes the rectangle that encloses an object\n",
    "- For each grid cell,\n",
    "  - it predicts B boundary boxes and each box has one box confidence score,\n",
    "  - it detects one object only regardless of the number of boxes B,\n",
    "  - it predicts C conditional class probabilities (one per class for the likeliness of the object class).\n",
    "\n",
    "<img width=500 src=\"images/1*m8p5lhWdFDdapEFa2zUtIA.jpeg\"/>\n",
    "<center><a href=\"https://arxiv.org/pdf/1506.02640v5.pdf\" style=\"color: lightgrey\">Credit</a></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bounding boxes:\n",
    "- Each bounding box contains 5 elements: $(x, y, w, h)$ and a box confidence score.\n",
    "- The (x, y) coordinates represent the center of the box, relative to the grid cell location. These coordinates are normalized to fall between 0 and 1. The (w, h) box dimensions are also normalized to [0, 1], relative to the image size.\n",
    "- The confidence score reflects how likely the box contains an object (objectness) and how accurate is the bounding box. If no object exists in that cell, the confidence score should be zero."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conditional class probabilities:\n",
    "- The conditional class probability is the probability that the detected object belongs to a particular class (one probability per category for each cell)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation:\n",
    "- The class confidence score for each prediction box measures the confidence on both the classification and the localization\n",
    "- If no object exists in that cell, the confidence score should be zero. Otherwise we want the confidence score to equal the intersection over union (IOU) between the predicted box and the ground truth.\n",
    "\n",
    "<img width=800 src=\"images/1*0IPktA65WxOBfP_ULQWcmw.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Intersection over union (IOU):\n",
    "- Intersection over Union, or Jaccard index, is an evaluation metric used to measure the accuracy of an object detector\n",
    "- It computes size of intersection and divide it by the union. More generally, IOU is a measure of the overlap between two bounding boxes.\n",
    "- The higher the IOU the better is the accuracy\n",
    "- An IOU score > 0.5 is normally considered as a true positive\n",
    "\n",
    "<img width=500 src=\"images/iou.png\"/>\n",
    "<center><a href=\"https://www.coursera.org/learn/convolutional-neural-networks\" style=\"color: lightgrey\">Credit</a></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Non-maximum suppression (NMS):\n",
    "- NMS eliminates some candidates that are in fact different detections of the same object, without removing the candidates for different objects.\n",
    "\n",
    "<img width=300 src=\"images/nms_algo.jpg\"/>\n",
    "<center><a href=\"https://leonardoaraujosantos.gitbooks.io/artificial-inteligence/content/single-shot-detectors/yolo.html\" style=\"color: lightgrey\">Credit</a></center>\n",
    "\n",
    "- For each object class:\n",
    "  - Discard all those bounding boxes where probability of object being present is below some threshold (0.6)\n",
    "  - Take the bounding box with the highest score among candidates\n",
    "  - Discard any remaining bounding boxes with IOU value above some threshold (0.5)\n",
    "- Non-maximal suppression adds 2-3% in mAP."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loss function:\n",
    "- YOLO uses sum-squared error between the predictions and the ground truth to calculate loss. The loss function composes of\n",
    "  - the classification loss,\n",
    "  - the localization loss (errors between the predicted boundary box and the ground truth),\n",
    "  - the confidence loss (the objectness of the box).\n",
    "- We can put more emphasis on any of the terms by multiplying it with a factor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pros:\n",
    "- Fast. Good for real-time processing.\n",
    "- Predictions (object locations and classes) are made from one single network. Can be trained end-to-end to improve accuracy.\n",
    "- YOLO is more generalized. It outperforms other methods when generalizing from natural images to other domains like artwork.\n",
    "- Region proposal methods limit the classifier to the specific region. YOLO accesses to the whole image in predicting boundaries. With the additional context, YOLO demonstrates fewer false positives in background areas.\n",
    "- YOLO detects one object per grid cell. It enforces spatial diversity in making predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### YOLOv2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Add BatchNorm to convolutional layers\n",
    "- Use anchor boxes like Faster-RCNN, the classification is done per-box shape, instead of per each grid-cell.\n",
    "- Instead of manually choosing the box shape, use K-means.\n",
    "- Train the network at multiple scales (224x224 then 448x448), as the network is now fully convolutional (no FC layer) this is easy to do. \n",
    "- Combine labels in different datasets to form a tree-like structure WordTree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Anchor boxes:\n",
    "- After doing some clustering studies on ground truth labels, it turns out that most bounding boxes have certain height-width ratios. So instead of directly predicting a bounding box, YOLOv2 (and v3) predict off-sets from a predetermined set of boxes with particular height-width ratios - those predetermined set of boxes are the anchor boxes.\n",
    "- Anchor box makes it possible for the YOLOv2 algorithm to detect multiple objects centered in one grid cell\n",
    "- The idea of anchor box adds one more “dimension” to the output labels by pre-defining a number of anchor boxes. We move the class prediction from the cell level to the boundary box level. Now, each prediction includes 4 parameters for the boundary box, 1 box confidence score (objectness) and class probabilities.\n",
    "\n",
    "<img width=500 src=\"images/0*WGFK1mm-f9HF-vO3.jpg\"/>\n",
    "\n",
    "- The anchor boxes need to be predefined either by hand or by using an algorithm such as K-Means (where distance is IOU)\n",
    "\n",
    "<img width=300 src=\"images/0*SX5qJIuV44XvBXju.jpg\"/>\n",
    "<center><a href=\"https://www.coursera.org/learn/convolutional-neural-networks\" style=\"color: lightgrey\">Credit</a></center>\n",
    "\n",
    "- One reason for choosing a variety of anchor box shapes is the similarity of one object’s bounding box shape to the shape of the anchor box\n",
    "- Another reason is to allow the model to specialize better\n",
    "- Anchor boxes decrease mAP slightly from 69.5 to 69.2 but the recall improves from 81% to 88%. i.e. even the accuracy is slightly decreased but it increases the chances of detecting all the ground truth objects."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pros:\n",
    "- Faster\n",
    "- More Accurate (73.4 mAP on Pascal dataset)\n",
    "- Can detect up to 9000 classes (20 before)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### YOLOv3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img width=700 src=\"images/1*d4Eg17IVJ0L41e7CTWLLSg.png\"/>\n",
    "<center><a href=\"https://towardsdatascience.com/yolo-v3-object-detection-53fb7d3bfe6b\" style=\"color: lightgrey\">Credit</a></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Detection at three scales:\n",
    "- The most salient feature of v3 is that it makes prediction at three scales, which are precisely given by downsampling the dimensions of the input image by 32, 16 and 8 respectively. \n",
    "- The detection is done by applying 1x1 detection kernels on feature maps of three different sizes at three different places in the network. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Detecting smaller objects:\n",
    "- Detections at different layers helps address the issue of detecting small objects, a frequent complaint with YOLOv2.\n",
    "- The upsampled layers concatenated with the previous layers help preserve the fine grained features which help in detecting small objects."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### More anchor boxes:\n",
    "- YOLOv3, in total uses 9 anchor boxes. Three for each scale. If you’re training YOLO on your own dataset, you should go about using K-Means clustering to generate 9 anchors.\n",
    "- YOLOv3 predicts 10x the number of boxes predicted by YOLOv2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic regression:\n",
    "- Squared errors have been replaced by cross-entropy error terms. In other words, object confidence and class predictions are now predicted through logistic regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multilabel classification:\n",
    "- YOLOv3 now performs multilabel classification for objects detected in images.\n",
    "- Softmaxing classes rests on the assumption that classes are mutually exclusive, or in simple words, if an object belongs to one class, then it cannot belong to the other. However, when we have classes like Person and Women in a dataset, then the above assumption fails."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pros:\n",
    "- YOLOv3 performs at par with other state of art detectors like RetinaNet, while being considerably faster\n",
    "\n",
    "<img width=450 src=\"images/1*YpNE9OQeshABhBgjyEXlLA.png\"/>\n",
    "<center><a href=\"https://towardsdatascience.com/yolo-v3-object-detection-53fb7d3bfe6b\" style=\"color: lightgrey\">Credit</a></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [You Only Look Once: Unified, Real-Time Object Detection (2015)](https://arxiv.org/pdf/1506.02640.pdf)\n",
    "- [YOLO9000: Better, Faster, Stronger (2016)](https://arxiv.org/pdf/1612.08242.pdf)\n",
    "- [YOLOv3: An Incremental Improvement (2018)](https://arxiv.org/pdf/1804.02767.pdf)\n",
    "- [Real-time Object Detection with YOLO, YOLOv2 and now YOLOv3](https://medium.com/@jonathan_hui/real-time-object-detection-with-yolo-yolov2-28b1b93e2088)\n",
    "- [Gentle guide on how YOLO Object Localization works with Keras](https://hackernoon.com/gentle-guide-on-how-yolo-object-localization-works-with-keras-part-1-aec99277f56f)\n",
    "- [How to implement a YOLO (v3) object detector from scratch in PyTorch](https://blog.paperspace.com/how-to-implement-a-yolo-object-detector-in-pytorch/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
