{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Style Transfer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Neural style transfer is a technique used to generate images in the style of another image\n",
    "\n",
    "<img width=600 src=\"images/perspolis_vangogh.png\"/>\n",
    "<center><a href=\"https://github.com/tejaslodaya/neural-style-transfer\" style=\"color: lightgrey\">Credit</a></center>\n",
    "\n",
    "- Inception network/GoogLeNet seems to be a little bit less well-suited for style transfer than the VGG network typically used for style transfer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The first published paper on neural style transfer used an optimization technique — that is, starting off with a random noise image and making it more and more desirable with every “training” iteration of the neural network.\n",
    "- This technique is used to take three images, a content image, a style reference image (such as an artwork by a famous painter), and the input image you want to style — and blend them together such that the input image is transformed to look like the content image, but “painted” in the style of the style image.\n",
    "- Since convnets pre-trained for image classification have already learnt to encode perceptual and semantic information that we need to measure semantic difference, they can be repurposed for the style transfer problem\n",
    "\n",
    "<img width=500 src=\"images/1*-bEkHF328n-S59iFnjTzag.png\"/>\n",
    "<center><a href=\"https://medium.com/data-science-group-iitr/artistic-style-transfer-with-convolutional-neural-network-7ce2476039fd\" style=\"color: lightgrey\">Credit</a></center>\n",
    "\n",
    "- We use the outputs of various intermediate layers to compute two types of losses: style loss (how close is the pastiche to the style image in style) and content loss (how close is the pastiche to the content image in content). Those losses are then minimized by directly changing the pastiche image. \n",
    "- Losses are based not on per-pixel differences between images, but instead in terms of higher level, more perceptual differences between them, which are captured in deeper layers.\n",
    "\n",
    "<img width=300 src=\"images/content-loss.png\"/>\n",
    "<img width=300 src=\"images/style-loss.png\"/>\n",
    "<center><a href=\"https://harishnarayanan.org/writing/artistic-style-transfer/\" style=\"color: lightgrey\">Credit</a></center>\n",
    "\n",
    "- The relative importance of these terms is determined by a set of scalar weights: $\\alpha$ and $\\beta$.\n",
    "\n",
    "<img width=500 src=\"images/1*INwW0Apz4wUpDS9jetJ7xQ.jpeg\"/>\n",
    "\n",
    "- Applying this to early layers in the network would capture some of the finer textures contained within the image whereas applying this to deeper layers would capture more higher-level elements of the image’s style.\n",
    "- The best results are achieved by a combination of many different layers from the network.\n",
    "\n",
    "<img width=600 src=\"images/1*YHpizJPE2QzXLPUVbD28Tg.png\"/>\n",
    "<center><a href=\"https://medium.com/data-science-group-iitr/artistic-style-transfer-with-convolutional-neural-network-7ce2476039fd\" style=\"color: lightgrey\">Credit</a></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Content loss:\n",
    "- Pass both the pastiche image and the content image through some layers of an image classification network and find the Euclidean distance between the intermediate representations of those images\n",
    "\n",
    "<img width=500 src=\"images/1*34xPuexhGCHT7xZ17wVvDQ.jpeg\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Style loss:\n",
    "- The style loss is the distance between the Gram matrices of the pastiche and style.\n",
    "- A Gram matrix results from multiplying a matrix with the transpose of itself.\n",
    "- The terms of the Gram matrix are proportional to the covariances of corresponding sets of features (channels), and thus captures information about which features tend to activate together.\n",
    "- By only capturing these aggregate statistics across the image, they are blind to the specific arrangement of objects inside the image. This is what allows them to capture information about style independent of content.\n",
    "\n",
    "<img width=500 src=\"images/1*IoozR3xGzaSqtEqGEKcWMQ.jpeg\"/>\n",
    "\n",
    "- There is also a possibility of assigning different weightages to the style loss at each layer.\n",
    "\n",
    "<img width=400 src=\"images/1*n7wIYY399mOdO9jJGM6aoA.jpeg\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [A Neural Algorithm of Artistic Style (2015)](https://arxiv.org/pdf/1508.06576)\n",
    "- [Convolutional neural networks for artistic style transfer](https://harishnarayanan.org/writing/artistic-style-transfer/)\n",
    "- [Neural Artistic Style Transfer: A Comprehensive Look](https://medium.com/artists-and-machine-intelligence/neural-artistic-style-transfer-a-comprehensive-look-f54d8649c199)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
