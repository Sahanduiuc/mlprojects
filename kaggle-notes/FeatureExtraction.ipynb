{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Group-based features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Some datasets such as transactional data have multiple rows for an instance.\n",
    "- Treat data points as dependent on each other.\n",
    "- Group by a categorical feature and calculate various statistics such as sum and mean (e.g., mean encoding).\n",
    "- To group by a numeric feature apply binning. Binning can be applied on both categorical and numerical data to make the model more robust and prevent overfitting, however, it has a cost to the performance.\n",
    "- For aggregating categorical features either: \n",
    "    - Select the label with the highest frequency\n",
    "    - Make a pivot table\n",
    "    - Apply *groupby* operation after one-hot-transforming the feature."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distance-based features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Perform a *groupby* operation on instance neighborhoods (not only geographical).\n",
    "- Explicit group is not needed\n",
    "- More flexible but harder to implement:\n",
    "    - (optional) Mean encode all variables to create a homogeneous feature space.\n",
    "    - Calculate $N$ nearest neighbors with some distance metric (e.g., Bray-Curtis). \n",
    "    - Calculate various statistics based on the nearest $K$ neighbors.\n",
    "    - Examples:\n",
    "        - Mean target of nearest 5, 10, 15, 500, 2000 neighbors\n",
    "        - Mean distance to 10 closest neighbors (with target 0/1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature interactions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Construct combinations of features in order to incorporate the knowledge into a model.\n",
    "- For categorical features:\n",
    "    - Concatenate them by using a delimiter.\n",
    "    - Or vectorize into a real-valued representation and then apply the operation.\n",
    "- There are $N^2*M$ possible interactions for $N$ features and $M$ operations:\n",
    "    - Reduce them by feature selection (e.g., with RF) or matrix factorization.\n",
    "- Such approach can be generalized for higher orders.\n",
    "- Due to the fact that number of features grows rapidly with order, they are often constructed semi-manually.\n",
    "- Extract interactions via decision trees by using indices of the leafs (*model.apply*).\n",
    "    - Two factors that are split in succession might indicate an interaction.\n",
    "    - [Facebook Research's paper about extracting categorical features from trees](https://research.fb.com/publications/practical-lessons-from-predicting-clicks-on-ads-at-facebook/)\n",
    "    - [Example: Feature transformations with ensembles of trees (sklearn)](http://scikit-learn.org/stable/auto_examples/ensemble/plot_feature_transformation.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matrix factorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Matrix factorization is a generic approach for dimensionality reduction and feature extraction.\n",
    "- Can provide additional diversity which is good for ensembles.\n",
    "- It is a lossy transformation which efficiency depends on the task and the number of latent factors.\n",
    "- The same transformation tricks as for linear models should be used.\n",
    "- The same parameters should be used throughout the dataset as it's a trainable transformation.\n",
    "- [Overview of Matrix Decomposition methods (sklearn)](http://scikit-learn.org/stable/modules/decomposition.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PCA:\n",
    "- PCA (Principal Component Analysis) performs a linear mapping of the data to a lower-dimensional space in such a way that the variance of the data in the low-dimensional representation is maximized. It does so by calculating the eigenvectors from the covariance matrix.\n",
    "- Can drop the least important feature while still retaining the most valuable parts.\n",
    "- Each of the new features or components created after PCA are all independent of one another.\n",
    "- Mainly applied to dense data.\n",
    "- Limitations:\n",
    "    - PCA is a linear algorithm (not able to interpret complex polynomial relationships between features).\n",
    "    - Visualization and interpretation difficulties.\n",
    "    - Strongly focused on variance which may not correlate with predictive power."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SVD:\n",
    "- SVD (Singular Value Decomposition) is a factorization of a matrix.\n",
    "- Same advantages and limitations as for PCA but faster.\n",
    "- `TruncatedSVD` can be applied to sparse matrices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NMF:\n",
    "- NMF (Nonnegative Matrix Factorization) is a state of the art feature extraction algorithm.\n",
    "- Automatically extracts sparse and meaningful features from a set of nonnegative data vectors (counts-like data).\n",
    "- NMF decomposes a data matrix $V$ into the product of two lower rank matrices $W$ and $H$ so that $V\\approx{W*H}$.\n",
    "\n",
    "<center><img width=550 src=\"images/holdout.png\"/></center>\n",
    "<center><a href=\"http://alexhwilliams.info/itsneuronalblog/2018/02/26/crossval/\" style=\"color: lightgrey\">Credit</a></center>\n",
    "\n",
    "- Primarily used for recommender systems and text mining.\n",
    "- Provides an additive basis to represent the data.\n",
    "- Results are easier to interpret than SVD.\n",
    "- Transforms data in a way best-suited for tree-based models.\n",
    "- NMF typically benefits from normalization.\n",
    "- Limitations:\n",
    "    - As opposed to the unconstrained problem which can be solved efficiently using the SVD, NMF is NP-hard. \n",
    "        - Fortunately there are heuristic approximations.\n",
    "    - There is no guarantee to be a single unique decomposition.\n",
    "    - Itâ€™s hard to know how to choose the factorisation rank $r$. \n",
    "        - Some approaches include trial and error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## t-SNE\n",
    "- t-SNE (t-Distributed Stochastic Neighbor Embedding) is a tool to visualize high-dimensional data.\n",
    "- Unlike PCA, is not a linear projection but allows to capture a non-linear structure.\n",
    "\n",
    "<center><img width=250 src=\"images/PCASwiss.png\"/></center>\n",
    "<center><a href=\"https://www.biostars.org/p/295174/\" style=\"color: lightgrey\">Credit</a></center>\n",
    "\n",
    "- Projects points from high to low-dimensional space by preserving the relative distance between them.\n",
    "- Optimizes the embeddings directly using gradient descent.\n",
    "- The principal components can be used as features.\n",
    "- Mainly a data exploration and visualization technique.\n",
    "- Apply t-SNE to concatenation of train and test and split projection back.\n",
    "- Limitations:\n",
    "    - Computationally expensive and can take several hours on million-sample datasets :\n",
    "        - It is common to do dimensionality reduction before projection.\n",
    "        - Use stand-alone implementation `tsne` for faster speed.\n",
    "    - Sometimes it works well for visualization but not for dimensionality reduction.\n",
    "    - Results strongly depend on hyperparameters (perplexity): \n",
    "        - Good practice is to use several projections with different perplexities (5-100).\n",
    "        - [Example: tSNE with different perplexities (sklearn)](http://scikit-learn.org/stable/auto_examples/manifold/plot_t_sne_perplexity.html#sphx-glr-auto-examples-manifold-plot-t-sne-perplexity-py)\n",
    "    - Due to stochastic nature, tSNE may provide different projects for the same configuration.\n",
    "    - PCA it is a mathematical technique, but t-SNE is a probabilistic one.\n",
    "    - There is the risk of getting stuck in local minima.\n",
    "- [Multicore t-SNE implementation](https://github.com/DmitryUlyanov/Multicore-TSNE)\n",
    "- [Comparison of Manifold Learning methods (sklearn)](http://scikit-learn.org/stable/auto_examples/manifold/plot_compare_methods.html)\n",
    "- [How to Use t-SNE Effectively (distill.pub blog)](https://distill.pub/2016/misread-tsne/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
