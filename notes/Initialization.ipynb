{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameter initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- For deep networks, we can use a heuristic to initialize the weights depending on the non-linear activation function\n",
    "- [ON THE SELECTION OF INITIALIZATION AND ACTIVATION FUNCTION FOR DEEP NEURAL NETWORKS (2018)](https://arxiv.org/pdf/1805.08266.pdf)\n",
    "- [Usage of initializers](https://keras.io/initializers/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zero initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Initializing weights to zero makes the hidden units symmetric and continues for all the iterations, which makes the model equivalent to a linear model\n",
    "- It is important to note that setting biases to 0 will not create any troubles as non-zero weights take care of breaking the symmetry"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Generate random sample of weights from a Gaussian distribution having mean 0 and a standard deviation of 1.\n",
    "- This serves the process of symmetry-breaking and gives much better accuracy. \n",
    "\n",
    "<img width=400 src=\"images/sigmoid.png\"/>\n",
    "<center><a href=\"https://mnsgrg.com/2017/12/21/xavier-initialization/\" style=\"color: lightgrey\">Credit</a></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vanishing gradients:\n",
    "- The weight update is minor and results in slower convergence. This makes the optimization of the loss function slow. In the worst case, this may completely stop the neural network from training further.\n",
    "- More specifically, in case of sigmoid and tanh, if weights are large, then the gradients will be vanishingly small.\n",
    "- With ReLU vanishing gradients are generally not a problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exploding gradients:\n",
    "- This may result in oscillating around the minima or even overshooting the optimum again and again and the model will never learn. Another impact of exploding gradients is that huge values of the gradients may cause number overflow resulting in incorrect computations or introductions of NaN’s. This might also lead to the loss taking the value NaN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Xavier initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Xavier initialization method tries to initialize weights with a smarter value, such that neurons won’t start training in saturation.\n",
    "- Glorot and Bengio (2009) suggested to initialize the weights from a distribution with zero mean and variance:\n",
    "\n",
    "<img width=300 src=\"images/Screen Shot 2018-12-05 at 15.23.32.png\"/>\n",
    "\n",
    "- The weights are still random but differ in range depending on the size of the previous layer of neurons\n",
    "- Xavier initialization makes sure the weights are ‘just right’, keeping the signal in a reasonable range of values through many layers.\n",
    "- Helps in attaining a global minimum of the cost function faster and more efficiently.\n",
    "\n",
    "<img width=800 src=\"images/training-losses.png\"/>\n",
    "<center><a href=\"https://intoli.com/blog/neural-network-initialization/\" style=\"color: lightgrey\">Credit</a></center>\n",
    "\n",
    "- Likewise if using Tanh activation function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [Understanding the difficulty of training deep feedforward neural networks (2010)](http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## He et al. initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- It draws samples from a truncated normal distribution centered on 0 with variance defined below, where $n_{in}$ is the number of input units in the weight tensor.\n",
    "\n",
    "<img width=200 src=\"images/Screen Shot 2018-12-05 at 15.10.10.png\"/>\n",
    "\n",
    "- Likewise if using ReLu activation function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification (2015)](https://arxiv.org/pdf/1502.01852v1.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
