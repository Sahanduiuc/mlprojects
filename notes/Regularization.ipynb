{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- This is a form of regression that penalizes the weights of the nodes and shrinks them towards zero. \n",
    "- In other words, this technique discourages learning a more complex or flexible model, so as to avoid the risk of overfitting.\n",
    "- It forces the downstream hidden units not to rely too much on the previous units by introducing noise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Overfitting:\n",
    "- The concept of regularization plays an important role in preventing overfitting\n",
    "- The primary reason overfitting happens is because the model learns even the tiniest details (noise) present in the data\n",
    "- In practice, it is always better to use regularization methods to control overfitting instead of the number of neurons.\n",
    "\n",
    "<img width=500 src=\"images/layer_sizes.jpeg\"/>\n",
    "<img width=500 src=\"images/reg_strengths.jpeg\"/>\n",
    "<center><a href=\"http://cs231n.github.io/neural-networks-1/\" style=\"color: lightgrey\">Credit</a></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Examples of regularization:\n",
    "- K-means: limiting the splits to avoid redundant classes\n",
    "- Random forests: limiting the tree depth, limiting new features (branches)\n",
    "- Neural networks: limiting the model complexity (weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lasso (L1) and Ridge regression (L2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- If model complexity is a function of weights, a feature weight with a high absolute value is more complex\n",
    "- Due to the addition of a regularization term to the loss function, the values of weight matrices decrease because it assumes that a neural network with smaller weight matrices leads to simpler models\n",
    "- If lambda is very large then it will add too much weight and it will lead to under-fitting. Having said that it’s important how lambda is chosen. This technique works very well to avoid over-fitting issue."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Difference:\n",
    "- The difference between the L1 and L2 is just that L2 is the sum of the square of the weights, while L1 is just the sum of the weights.\n",
    "- L1 and L2 regularization methods are also combined in what is called elastic net regularization.\n",
    "\n",
    "<img width=400 src=\"images/1*JP9VzwxsRXjocb3WgVOTxA.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### L1 benefits:\n",
    "- Built-in feature selection is frequently mentioned as a useful property of the L1-norm, which the L2-norm does not. This is actually a result of the L1-norm, which tends to produces sparse coefficients.\n",
    "- L1 norm does not place emphasis on the outliers. It is robust to them. There is no square term to blow up the error difference of the outliers and the minimization algorithm keeps its focus on the data instead of the outliers.\n",
    "- The fact that L1 can be used for noisy data is very appealing and is exploited a lot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### L2 benefits:\n",
    "- The general frustration with the L1 norm is usually directed toward the fact that it is not differentiable and is not attractive to use in an optimization setting.\n",
    "- L2 scores are usually overall better than L1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weight decay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- L2 regularization and weight decay are not identical\n",
    "- L2 regularization is not effective in Adam\n",
    "- Weight decay is equally effective in both SGD and Adam\n",
    "- Optimal weight decay depends on the total number of batch passes/weight updates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [AdamW and Super-convergence is now the fastest way to train neural nets](https://www.fast.ai/2018/07/02/adam-weight-decay/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The most frequently used regularization technique in the field of deep learning\n",
    "- The key idea is to randomly drop units (along with their connections) from the neural network during training. This prevents units from co-adapting too much. Also divide each dropout layer by probability to keep the same expected value for the activation.\n",
    "- Apply dropout both during forward and backward propagation. \n",
    "- Disable dropout during testing\n",
    "- If I have to explain drop-out to a 6-year-old, this is how: Imagine a scenario, in a classroom, a teacher asks some questions but always same two kids are answering, immediately. Now, the teacher asks them to stay quiet for some time and let other pupils participate. This way other students get to learn better. Maybe they answer wrong, but the teacher can correct them(weight updates). This way the whole class(layer) learn about a topic better.\n",
    "\n",
    "<img width=500 src=\"images/1*IrdJ5PghD9YoOyVAQ73MJw.gif\"/>\n",
    "<center><a href=\"https://chatbotslife.com/regularization-in-deep-learning-f649a45d6e0\" style=\"color: lightgrey\">Credit</a></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observations:\n",
    "- Dropout forces a neural network to learn more robust features that are useful in conjunction with many different random subsets of the other neurons.\n",
    "- Dropout roughly doubles the number of iterations required to converge. However, training time for each epoch is less.\n",
    "- With H hidden units, each of which can be dropped, we have $2^H$ possible models.\n",
    "- It can also be thought of as an ensemble technique in machine learning. Ensemble models usually perform better than a single model as they capture more randomness."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tips:\n",
    "- Generally, use a small dropout value of 20%-50% of neurons with 20% providing a good starting point. A probability too low has minimal effect and a value too high results in under-learning by the network.\n",
    "- Use a larger network. You are likely to get better performance when dropout is used on a larger network, giving the model more of an opportunity to learn independent representations.\n",
    "- Use dropout on incoming (visible) as well as hidden units.\n",
    "- Use a large learning rate with decay and a large momentum."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Suppose we are building an image classification model and are lacking the requisite data due to various reasons. \n",
    "- Data augmentation refers to randomly changing the images in ways that shouldn't impact their interpretation, such as horizontal flipping, zooming, and rotating, to effectively create more data.\n",
    "- These can potentially help us get more training data and hence reduce overfitting.\n",
    "- Commonly data augmentation and training tasks are run on parallel CPU threads\n",
    "\n",
    "<img width=500 src=\"images/data-augmentation.png\"/>\n",
    "<center><a href=\"https://www.analyticsvidhya.com/blog/2018/04/fundamentals-deep-learning-regularization-techniques/\" style=\"color: lightgrey\">Credit</a></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Image size:\n",
    "- One effective way to synthesize more data is to downscale/upscale images during training.\n",
    "- Fully-convolutional networks (FCN) as well as networks with Global Average Pooling (GAP) can work regardless of the original image size, without requiring any fixed number of units at any stage, given that all connections are local."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Early stopping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Early stopping is a kind of cross-validation strategy where we keep one part of the training set as the validation set. \n",
    "- When we see that the performance on the validation set is getting worse, we immediately stop the training on the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Method:\n",
    "- Split the training data into a training set and a validation set􏰌\n",
    "- Train only on the training set and evaluate the per-􏰍example error on the validation set once in a while􏰌\n",
    "- Stop training as soon as the error on the validation set is higher than it was for the last time it was checked􏰎\n",
    "- Use the weights the network had in that previous step as the result of the training run􏰎"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [Early Stopping - but when􏰀 (1997)](http://page.mi.fu-berlin.de/prechelt/Biblio/stop_tricks1997.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
